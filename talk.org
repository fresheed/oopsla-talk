* Talk - v0
  1. titular slide
     Good morning, colleagues. My name is Egor Namakonov, (_? I am a PhD student from ..._), and today I'll present the project we've made with Ori Lahav, Anton Podkopaev, Jonas Oberhauser and Viktor Vafeiadis.
     The main concern of the project is establishing the sufficient conditions for program termination given that these programs operate under non-trivial _memory models_. 
  2. +?Outline and contributions (? is the outline needed?)+ Contributions
     First, we describe the reasons why the common-sense reasoning is not sufficient for proving the termination of spinlock-alike programs, _or, in other words, why the executions of such programs can be not fair_. For that, we describe the existing operational memory models definitions and propose to extend them in such a way that the resulting models guarantee _fairness_.
     Second, we observe that, instead of using the ad-hoc extension for each of these operational definitions, it is possible to extend their _declarative_ counterparts with a unified fairness requirement.
     Third, we demonstrate how the unified fairness requirement can be used to prove the termination of lock algorithms.
     Also, _key results_ of the paper are formalized in Coq.
  3. Spinlock terminates under sequential consistency  _add about thread fairness?
     And now let's take a look at the following example. This is a spinlock algorithm - a simple implementation of the mutual exclusion _for the critical section (which is empty in our example)_. The status of the critical section for which client threads compete is represented by the shared variable 'l', where the value '1' means that the section is currently occupied and the value '0' means that the section is free. 
     Each thread that wants to enter the critical section executes the 'lock' procedure during which it tries to atomically change the value of 'l' from zero to one. If the change succeeds, the thread proceeds, otherwise it tries again. After executing the critical section a thread simply writes zero to 'l', thus allowing other threads to enter it.
     A natural condition for such algorithms is to require that every client will eventually enter the critical section. This is guaranteed by the termination of the 'lock' procedure.
     And for an intuitive interleaving concurrency semantics (aka sequential consistency) it is not hard to see that this requirement is indeed satisfied. The interleaving semantics states that the execution of the concurrent program can be represented _as ......_. The only requirement we need is that every active thread is eventually scheduled for the execution. This requirement _is also known_ as 'thread fairness'. Let's see how our program operates under such semantics.
     _Как может происходить исполнение программы? Планировщик выберет один из потоков (пусть это будет первый), и тот исполнит свою первую инструкцию - атомарную замену - и успешно войдёт в критическую секцию. После этого может произойти переключение контекстов - второй поток будет крутиться в цикле ожидания, пытаясь исполнить атомарную замену. Но, так как планировщик справедливый, рано или поздно он обратно переключится на первый поток, и тот дойдёт до unlock и запишет в l ноль. После этого второй поток также продолжит и завершить своё исполнение._
     To summarize, we required that every thread is eventually scheduled, and from that it followed that the second thread will eventually see the last write from the first thread.
  4. Spinlock may not terminate under weak memory
     But the situation is more complicated with the execution on the real hardware. Namely, the structure of the memory becomes is no more trivial. Take, for example, the x86-TSO model. In order to optimize memory accesses it buffers writes. That is, each write operation first goes to a thread-local buffer. The propagation from that buffer to the main memory can be delayed, and during this delay other threads reading from the memory will observe the obsolete value. Such behaviors are not possible under sequential consistency, and that's why x86-TSO and similar models are said to have weak memory models. 
     Let's see how it affects the previous reasoning. _Первый поток вошёл в критическую секцию, второй поток тем временем крутится в цикле и ожидает её освобождения. Мы снова переключаемся на первый поток и производим запись нуля, которая говорит, что критическая секция свободна. Но эта запись попадает в буфер первого потока. И пока она остаётся в нём, второй поток будет думать, что критическая секция занята._
  5. We need to require both thread and memory fairness
     As we can see, the thread fairness alone is not sufficient to guarantee the termination anymore. We also need to require that each buffered write is eventually propagated to the main memory. With this condition, the second thread will in turn eventually read the latest write of zero and proceed.
     This additional requirement for the eventual writes propagation resembles one for the eventual thread scheduling. That's why we call this condition memory fairness, similar to thread fairness.
  6. Memory fairness definitions vary for different memory models
     And similar requirements are need to be imposed on other memory models as well. For example, there is a 'release-acquire' memory model which can be thought of as a simplified C++ memory model. It is a message-based model, in which the memory is represented as a bag of messages, and a read operation non-deterministically picks one of them.
     Under the RA, the unfair behavior we discussed before can arise if the second thread always reads the old message. That is, the memory fairness for the RA should require that a thread should eventually read a newer message if there is one.
  7. Unifying the memory fairness by abstracting over memory model
     The process above can be applied to any operational memory model. That is, we determine the possible cause of non-fair execution and require from the memory model that it cannot happen infinitely. This leads to a bunch of different fairness definitions, which leads to excessive proof efforts. Namely, we'd have to repeat the similar termination proof for all the memory models. And that leads us to a question if there is a uniform memory fairness definition applicable to all memory models. _we recall what declarative definitions are?_
     To find one, let's abstract over a memory model and see what happend during the program execution in general. Simply speaking, each thread's execution can be represented as a sequence of memory access events that occur during a particular instruction's execution. _more comments and explanations_ And we're interested whether it's possible to have an infinite amount of reads observing the value 1. 
     Note that we're not particularly interested in how exactly the thread interleaving goes. That is, there is no need for total order on all events; it is only sufficient to order events belonging to the same thread, like this. These edges correspond to program order of a thread's instructions.
     Note that the value observed by a read event cannot be determined by a thread itself. That is, in theory it is possible to have a read event observing a value 55 because some other thread did the corresponding write. But when we know how each thread executes, we can justify all read events by relating them to some write. We can represent it with "reads-from" edge. And this is how declarative memory models work: an execution is represented as a set of events related by multiple relations. For example, most of memory models also account for so-called "modification order" on writes which, simply speaking, represents that one write overwrites another; for example, the non-initial write overwrites the initial one. _TSO intuition_ And the memory model itself is defined as a predicate on such graphs. For example, _difference between SC and TSO_.
  8. Memory fairness can be expressed as prefix-finiteness of a relation
     Let's see how an unfair execution's graph would look like. If 'lock' procedure in the second thread never terminates, it produces an infinite amount of read events each of which observes the value one. And obviously each of them reads from the corresponding write from the first thread.
     We can interpret the resulting graph in the following way. Each of these reads is performed before the final write of zero, because a read observes the write which is placed before the last one in the modification order. It means that this last write has an infinite amount of predecessors in the following composite relation. So, in order to prohibit such behavior, we need to require that this relation has a finite prefix.
     Actually, there are similar examples that demonstrate that we also need to require the finiteness of "modification order" prefix as well. So, as a result, we come up with the following memory fairness definition expressed as a conjunction of two prefix-finiteness conditions.
  9. The declarative memory fairness definition is _universal_
     We show that the obtained definition is indeed valid. Namely, we prove that for multiple memory models, their operational definitions extended with an ad-hoc fairness requirement are equivalent with a declarative definition extended with a uniform fairness requirement. We do that for such models as SC, TSO, RA and StrongCOH. These results are formalized in Coq. 
     Since the equivalence hold, we propose to use that definition for other models that don't have an operational counterpart, for example, RC11 which fixes some problems of the original C11 memory model. This proposal is also supported by the fact that the existing compilation scheme from RC11 to TSO and compiler optimizations for it remain valid even after requiring fairness. Also, the fairness condition is compatible with the extension of robustness theorems for the case of infinite executions, which we also prove in Coq.
  10. Memory fairness gives a practical way to prove termination
      For proving a program's termination, we propose a criteria derived from the original fairness definition. Namely, it implies that in an infinite busy-wait loop execution there will occur an iteration where the read observes the latest write in terms of modification order. For example, in our case that implies that eventually the second thread will observe the value zero, which should terminate the busy-wait. 
      This criteria is applicable for other locking algorithms as well. In the paper and the Coq development we use it to prove the termination of HMCS lock and the progress of the ticket lock. 
      _Also, the paper provides the explanation of HMCS lock non-terminating behavior observed by Oberhauser et al. recently._
  11. _Conclusion?_
      To sum up, our paper provides multiple operational memory fairness definitions as well as a uniform declarative one. These definitions are proved to be equivalent for major memory models definitions. And we show how to use the uniform memory fairness definition for the concurrent algorithms' verification. The key results are verified in Coq. 
      And that concludes my talk. Thank you for the attention, I'll be happy to answer your questions. 
* Talk - v1
  1. titular slide
     Good morning, colleagues. My name is Egor Namakonov, I am a PhD student from St Petersburg University and JetBrains Research, and today I'll present the project we've made with Ori Lahav, Anton Podkopaev, Jonas Oberhauser and Viktor Vafeiadis.
     The main concern of the project is establishing the sufficient termination conditions for programs which execution doesn't follow the intuitive concurrency semantics. 
  2. Spinlock terminates under sequential consistency
     Let's take a look at the following example. This is a spinlock algorithm which is used to implement a critical section (which is empty in our example). The status of the critical section for which client threads compete is represented by the shared variable 'l', where the value '1' means that the section is currently occupied and the value '0' means that the section is free. 
     Each thread that wants to enter the critical section executes the 'lock' procedure during which it tries to atomically change the value of 'l' from zero to one. If the change succeeds, the thread proceeds, otherwise it tries again. After executing the critical section a thread simply writes zero to 'l', thus allowing other threads to enter it.
     A natural condition for such algorithms is to require that every client will eventually enter the critical section. This is guaranteed by the termination of the 'lock' procedure.
     And for an intuitive interleaving concurrency semantics (aka sequential consistency) it is not hard to see that this requirement is indeed satisfied. The interleaving semantics states that the execution of the concurrent program can be represented as execution of single thread instructions interleaved with each other. The only requirement we need is that every active thread is eventually scheduled for the execution. This requirement is also known as /thread fairness/. Let's see how our program operates under such semantics.
     The scheduler picks one of threads (we assume it is the first one) which executes its first instruction - namely, compare-and-swap - and enters critical section. After that a context switch may occur, which will cause the second thread to repeatedly try and fail its compare-and-swap instruction. But since the scheduler is fair, eventually the first thread will be selected again, and so it will write zero to 'l'. After that, the second thread will enter the critical section and terminate as well. 
     To summarize, we required that every thread is eventually scheduled, and from that it followed that the second thread will eventually see the last write from the first thread.
  3. Spinlock may not terminate under /weak memory/
     But the situation is more complicated with the execution on the real hardware. Namely, the structure of the memory is no more trivial. Take, for example, the x86-TSO architecture. In order to optimize memory accesses it buffers writes. That is, each write operation first goes to a thread-local buffer. The propagation from that buffer to the main memory can be delayed, and during this delay other threads reading from the memory will observe the obsolete value. Such behaviors are not possible under sequential consistency, aka strong memory model, and that's why x86-TSO and similar architectures are said to have /weak memory models/. 
     Let's see how it affects the previous reasoning. The first thread enters the critical section, meanwhile the second one tries to enter it. Eventually we switch back to the first thread which writes zero to 'l' meaning that the critical section is free. But this write remains in the first thread's buffer. That means that the second thread doesn't observe the change in value and doesn't enter the critical section.
  4. We need to require both thread and memory fairness
     As we can see, the thread fairness alone is not sufficient to guarantee the termination anymore. We also need to require that each buffered write is eventually propagated to the main memory. With this condition, the second thread will eventually read the latest write of zero and proceed.
     This additional requirement for the eventual writes propagation resembles one for the eventual thread scheduling. That's why we call this condition memory fairness, similar to thread fairness.
  5. Establishing memory fairness for /declarative/ memory models
     This is a demonstration of a general approach of how to formalize a memory fairness requirement: we analyze the operational semantics, find the possible cause of unfair execution and require that eventually it should terminate. But such approach only applies to operational memory models, that is, those where the execution of a program corresponds to a sequence of an abstract machine states. Meanwhile, a lot of modern memory models are defined in another way, namely, declaratively. 
     Let's see how declarative memory models are defined. Simply speaking, each thread's execution can be represented as a sequence of memory access events that occur during a particular instruction's execution. (? more comments)
     Note that usually we're not particularly interested in how exactly the thread interleaving goes. That is, there is no need for total order on all events; it is only sufficient to order events belonging to the same thread, like this. These edges correspond to program order of a thread's instructions.
     Note that the value observed by a read event cannot be determined by a thread itself. That is, in theory it is possible to have a read event observing a value 55 because some other thread did the corresponding write. But when we know how each thread executes, we can justify all read events by relating them to some write. We can represent it with "reads-from" edge. And this is how declarative memory models work: an execution is represented as a set of events related by multiple relations. For example, most of memory models also account for so-called "modification order" which, simply speaking, represents that one write is being overwritten by another; for example, the non-initial write overwrites the initial one. And the memory model itself is defined as a predicate on such graphs. For example, the sequential consistency requires the absence of cycles made of po, rf and mo edges (? don't mention fr here yet). 
  6. Memory fairness can be expressed as prefix-finiteness of a relation
     Going back to fairness, let's see how an unfair execution's graph would look like. If 'lock' procedure in the second thread never terminates, it produces an infinite amount of read events each of which observes the value one. And obviously each of them reads from the corresponding write from the first thread.
     We can interpret the resulting graph in the following way. Each of these reads is performed before the final write of zero, because a read observes the write which is placed before the last one in the modification order. It means that this last write has an infinite amount of predecessors in the following composite relation. So, in order to prohibit such behavior, we need to require that this relation has a finite prefix.
     Actually, there are similar examples that demonstrate that we also need to require the finiteness of "modification order" prefix as well. So, as a result, we come up with the following memory fairness definition expressed as a conjunction of two prefix-finiteness conditions.
  7. The declarative memory fairness definition is universal
     We show that the obtained definition is indeed valid. Namely, we prove that for multiple memory models, their operational definitions extended with an ad-hoc fairness requirement are equivalent with a declarative definition extended with a uniform fairness requirement. We do that for such models as SC, TSO, RA and StrongCOH. We didn't pay attention to the last two, but they can be thought of as simplifications of C++ memory model. 
     These results are formalized in Coq. 
  8. Memory fairness gives a practical way to prove termination
     For proving a program's termination, we propose a criteria derived from the declarative fairness definition. Namely, it implies that in an infinite busy-wait loop execution there will occur an iteration where the read observes the latest write in terms of modification order. For example, in our case that implies that eventually the second thread will observe the value zero, which should terminate the busy-wait. 
     This criteria is applicable for other locking algorithms as well. In the paper and the Coq development we use it to prove the termination of HMCS lock and the progress of the ticket lock. 
  9. Conclusion and future work
     To sum up, our paper provides multiple operational memory fairness definitions as well as a uniform declarative one. These definitions are proved to be equivalent for major memory models definitions. And we show how to use the uniform memory fairness definition for the concurrent algorithms' verification. The key results are verified in Coq. 
     Regarding the future work, we propose to use that definition for other models, namely, RC11 which fixes some issues of the original C11 memory model. This proposal is also supported by the fact that the existing compilation scheme from RC11 to TSO and compiler optimizations for it remain valid even when fairness is required.
     Other models we're aiming to support are ARMv8- and Power-alike models under which another kinds of unfair behaviors arise, as we demonstrate in the paper. 
     And that concludes my talk. Thank you for the attention, I'll be happy to answer your questions.
